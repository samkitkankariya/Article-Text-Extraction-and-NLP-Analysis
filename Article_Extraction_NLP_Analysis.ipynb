{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LEoc0GAKqikZNkVFXYSWxnAkHlvUteH0",
      "authorship_tag": "ABX9TyMYMwJtpsCqoHZbQtBzZgmj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samkitkankariya/Article-Text-Extraction-and-NLP-Analysis/blob/main/Article_Extraction_NLP_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Iterate through each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract the text content from the HTML\n",
        "        heading = soup.find('h1', class_ = 'entry-title')\n",
        "        paragraphs = soup.find('div', class_ = 'td-post-content tagdiv-type')\n",
        "\n",
        "        # Save the content to a text file\n",
        "        with open(f'Extracted Files Data/{url_id}.txt', 'w') as f:\n",
        "            f.write(heading.text)\n",
        "            for p in paragraphs:\n",
        "                f.write(p.text)\n",
        "\n",
        "        print(f\"Data scraped successfully for URL_ID: {url_id}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data for URL_ID: {url_id} - {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for URL_ID: {url_id} - {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnA6EuZNvV14",
        "outputId": "43c68c7f-6645-4de1-d24f-4a2ae745b8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraped successfully for URL_ID: blackassign0001\n",
            "Data scraped successfully for URL_ID: blackassign0002\n",
            "Data scraped successfully for URL_ID: blackassign0003\n",
            "Data scraped successfully for URL_ID: blackassign0004\n",
            "Data scraped successfully for URL_ID: blackassign0005\n",
            "Data scraped successfully for URL_ID: blackassign0006\n",
            "Data scraped successfully for URL_ID: blackassign0007\n",
            "Data scraped successfully for URL_ID: blackassign0008\n",
            "Data scraped successfully for URL_ID: blackassign0009\n",
            "Data scraped successfully for URL_ID: blackassign0010\n",
            "Data scraped successfully for URL_ID: blackassign0011\n",
            "Data scraped successfully for URL_ID: blackassign0012\n",
            "Data scraped successfully for URL_ID: blackassign0013\n",
            "An error occurred for URL_ID: blackassign0014 - 'NoneType' object has no attribute 'text'\n",
            "Data scraped successfully for URL_ID: blackassign0015\n",
            "Data scraped successfully for URL_ID: blackassign0016\n",
            "Data scraped successfully for URL_ID: blackassign0017\n",
            "Data scraped successfully for URL_ID: blackassign0018\n",
            "Data scraped successfully for URL_ID: blackassign0019\n",
            "An error occurred for URL_ID: blackassign0020 - 'NoneType' object has no attribute 'text'\n",
            "Data scraped successfully for URL_ID: blackassign0021\n",
            "Data scraped successfully for URL_ID: blackassign0022\n",
            "Data scraped successfully for URL_ID: blackassign0023\n",
            "Data scraped successfully for URL_ID: blackassign0024\n",
            "Data scraped successfully for URL_ID: blackassign0025\n",
            "Data scraped successfully for URL_ID: blackassign0026\n",
            "Data scraped successfully for URL_ID: blackassign0027\n",
            "Data scraped successfully for URL_ID: blackassign0028\n",
            "An error occurred for URL_ID: blackassign0029 - 'NoneType' object has no attribute 'text'\n",
            "Data scraped successfully for URL_ID: blackassign0030\n",
            "Data scraped successfully for URL_ID: blackassign0031\n",
            "Data scraped successfully for URL_ID: blackassign0032\n",
            "Data scraped successfully for URL_ID: blackassign0033\n",
            "Data scraped successfully for URL_ID: blackassign0034\n",
            "Data scraped successfully for URL_ID: blackassign0035\n",
            "Error fetching data for URL_ID: blackassign0036 - 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Data scraped successfully for URL_ID: blackassign0037\n",
            "Data scraped successfully for URL_ID: blackassign0038\n",
            "Data scraped successfully for URL_ID: blackassign0039\n",
            "Data scraped successfully for URL_ID: blackassign0040\n",
            "Data scraped successfully for URL_ID: blackassign0041\n",
            "Data scraped successfully for URL_ID: blackassign0042\n",
            "An error occurred for URL_ID: blackassign0043 - 'NoneType' object has no attribute 'text'\n",
            "Data scraped successfully for URL_ID: blackassign0044\n",
            "Data scraped successfully for URL_ID: blackassign0045\n",
            "Data scraped successfully for URL_ID: blackassign0046\n",
            "Data scraped successfully for URL_ID: blackassign0047\n",
            "Data scraped successfully for URL_ID: blackassign0048\n",
            "Error fetching data for URL_ID: blackassign0049 - 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Data scraped successfully for URL_ID: blackassign0050\n",
            "Data scraped successfully for URL_ID: blackassign0051\n",
            "Data scraped successfully for URL_ID: blackassign0052\n",
            "Data scraped successfully for URL_ID: blackassign0053\n",
            "Data scraped successfully for URL_ID: blackassign0054\n",
            "Data scraped successfully for URL_ID: blackassign0055\n",
            "Data scraped successfully for URL_ID: blackassign0056\n",
            "Data scraped successfully for URL_ID: blackassign0057\n",
            "Data scraped successfully for URL_ID: blackassign0058\n",
            "Data scraped successfully for URL_ID: blackassign0059\n",
            "Data scraped successfully for URL_ID: blackassign0060\n",
            "Data scraped successfully for URL_ID: blackassign0061\n",
            "Data scraped successfully for URL_ID: blackassign0062\n",
            "Data scraped successfully for URL_ID: blackassign0063\n",
            "Data scraped successfully for URL_ID: blackassign0064\n",
            "Data scraped successfully for URL_ID: blackassign0065\n",
            "Data scraped successfully for URL_ID: blackassign0066\n",
            "Data scraped successfully for URL_ID: blackassign0067\n",
            "Data scraped successfully for URL_ID: blackassign0068\n",
            "Data scraped successfully for URL_ID: blackassign0069\n",
            "Data scraped successfully for URL_ID: blackassign0070\n",
            "Data scraped successfully for URL_ID: blackassign0071\n",
            "Data scraped successfully for URL_ID: blackassign0072\n",
            "Data scraped successfully for URL_ID: blackassign0073\n",
            "Data scraped successfully for URL_ID: blackassign0074\n",
            "Data scraped successfully for URL_ID: blackassign0075\n",
            "Data scraped successfully for URL_ID: blackassign0076\n",
            "Data scraped successfully for URL_ID: blackassign0077\n",
            "Data scraped successfully for URL_ID: blackassign0078\n",
            "Data scraped successfully for URL_ID: blackassign0079\n",
            "Data scraped successfully for URL_ID: blackassign0080\n",
            "Data scraped successfully for URL_ID: blackassign0081\n",
            "Data scraped successfully for URL_ID: blackassign0082\n",
            "An error occurred for URL_ID: blackassign0083 - 'NoneType' object has no attribute 'text'\n",
            "An error occurred for URL_ID: blackassign0084 - 'NoneType' object has no attribute 'text'\n",
            "Data scraped successfully for URL_ID: blackassign0085\n",
            "Data scraped successfully for URL_ID: blackassign0086\n",
            "Data scraped successfully for URL_ID: blackassign0087\n",
            "Data scraped successfully for URL_ID: blackassign0088\n",
            "Data scraped successfully for URL_ID: blackassign0089\n",
            "Data scraped successfully for URL_ID: blackassign0090\n",
            "Data scraped successfully for URL_ID: blackassign0091\n",
            "An error occurred for URL_ID: blackassign0092 - 'NoneType' object has no attribute 'text'\n",
            "Data scraped successfully for URL_ID: blackassign0093\n",
            "Data scraped successfully for URL_ID: blackassign0094\n",
            "Data scraped successfully for URL_ID: blackassign0095\n",
            "Data scraped successfully for URL_ID: blackassign0096\n",
            "Data scraped successfully for URL_ID: blackassign0097\n",
            "Data scraped successfully for URL_ID: blackassign0098\n",
            "An error occurred for URL_ID: blackassign0099 - 'NoneType' object has no attribute 'text'\n",
            "An error occurred for URL_ID: blackassign0100 - 'NoneType' object has no attribute 'text'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of indices to scrape\n",
        "# For the files which had error  - 'NoneType' object has no attribute 'text'\n",
        "indices_to_scrape = [13, 19, 28, 42,82,83,91,98,99]  # For the files which had error\n",
        "\n",
        "for index in indices_to_scrape:\n",
        "    url_id = df.iloc[index]['URL_ID']\n",
        "    url = df.iloc[index]['URL']\n",
        "\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract the text content from the HTML\n",
        "        heading = soup.h1.text\n",
        "        paragraphs = soup.find('div', class_ = 'td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
        "        p1 = paragraphs.div\n",
        "        # Save the content to a text file\n",
        "        with open(f'Extracted Files Data/{url_id}.txt', 'w') as f:\n",
        "            f.write(heading)\n",
        "            for p in p1:\n",
        "                f.write(p1.p.text)\n",
        "\n",
        "        print(f\"Data scraped successfully for URL_ID: {url_id}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data for URL_ID: {url_id} - {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for URL_ID: {url_id} - {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzPCDMgxyQDp",
        "outputId": "06409729-ca45-418f-afae-f25aa5473c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraped successfully for URL_ID: blackassign0014\n",
            "Data scraped successfully for URL_ID: blackassign0020\n",
            "Data scraped successfully for URL_ID: blackassign0029\n",
            "Data scraped successfully for URL_ID: blackassign0043\n",
            "Data scraped successfully for URL_ID: blackassign0083\n",
            "Data scraped successfully for URL_ID: blackassign0084\n",
            "Data scraped successfully for URL_ID: blackassign0092\n",
            "Data scraped successfully for URL_ID: blackassign0099\n",
            "Data scraped successfully for URL_ID: blackassign0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import io\n",
        "nltk.download('punkt')  # Ensure NLTK tokenizer is downloaded\n",
        "\n",
        "# Load stop words\n",
        "stop_words_folder = 'StopWords'\n",
        "stop_words = []\n",
        "for file_name in os.listdir(stop_words_folder):\n",
        "    with open(os.path.join(stop_words_folder, file_name), 'r',  encoding='latin-1') as f:\n",
        "        stop_words.extend(f.read().splitlines())\n",
        "\n",
        "# Load positive and negative word dictionaries\n",
        "positive_words_path = 'MasterDictionary/positive-words.txt'\n",
        "negative_words_path = 'MasterDictionary/negative-words.txt'\n",
        "\n",
        "def load_word_list(file_path, encoding='utf-8'):\n",
        "    with io.open(file_path, 'r', encoding=encoding) as file:\n",
        "        return file.read().splitlines()\n",
        "\n",
        "positive_words = load_word_list(positive_words_path, encoding='latin-1')\n",
        "negative_words = load_word_list(negative_words_path, encoding='latin-1')\n",
        "\n",
        "\n",
        "# Define a function for sentiment analysis and derived variables calculation\n",
        "def analyze_sentiment(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Perform sentiment analysis based on positive and negative dictionaries\n",
        "    positive_score = sum(1 for token in tokens if token.lower() in positive_words)\n",
        "    negative_score = sum(1 for token in tokens if token.lower() in negative_words)\n",
        "\n",
        "    # Calculate Polarity Score\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "\n",
        "    # Calculate Subjectivity Score\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
        "\n",
        "    # Calculate readability metrics\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    total_words = len(tokens)\n",
        "    total_sentences = len(sentences)\n",
        "    avg_sentence_length = total_words / total_sentences\n",
        "    complex_words = sum(1 for token in tokens if len(token) > 2)  # Assuming words with more than 2 syllables are complex\n",
        "    percentage_complex_words = complex_words / total_words\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = total_words / total_sentences\n",
        "\n",
        "    # Count syllables\n",
        "    def count_syllables(word):\n",
        "        count = 0\n",
        "        vowels = 'aeiouy'\n",
        "        word = word.lower()\n",
        "        if word.endswith(('es', 'ed')):\n",
        "            pass\n",
        "        else:\n",
        "            for index, letter in enumerate(word):\n",
        "                if letter in vowels and (index == 0 or word[index - 1] not in vowels):\n",
        "                    count += 1\n",
        "        return count\n",
        "\n",
        "    syllables_count = sum(count_syllables(token) for token in tokens)\n",
        "\n",
        "    # Count personal pronouns\n",
        "    personal_pronouns = ['i', 'we', 'my', 'ours', 'us']\n",
        "    personal_pronouns_count = sum(1 for token in tokens if token.lower() in personal_pronouns)\n",
        "\n",
        "    # Calculate average word length\n",
        "    avg_word_length = sum(len(token) for token in tokens) / total_words\n",
        "\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, \\\n",
        "           percentage_complex_words, fog_index, avg_words_per_sentence, syllables_count, personal_pronouns_count, avg_word_length\n",
        "\n",
        "# Create a DataFrame for storing output data\n",
        "output_df = pd.DataFrame(columns=['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE',\n",
        "                                   'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
        "                                   'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "                                   'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
        "\n",
        "# Iterate over each URL_ID and perform sentiment analysis and derived variables calculation\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    try:\n",
        "        # Read the text file content\n",
        "        with open(f'Extracted Files Data/{url_id}.txt', 'r') as text_file:\n",
        "            text = text_file.read()\n",
        "\n",
        "        # Perform sentiment analysis and derived variables calculation\n",
        "        positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, \\\n",
        "        percentage_complex_words, fog_index, avg_words_per_sentence, syllables_count, personal_pronouns_count, avg_word_length \\\n",
        "        = analyze_sentiment(text)\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        new_row = ({'URL_ID': url_id, 'URL': url,'POSITIVE SCORE': positive_score,\n",
        "                                      'NEGATIVE SCORE': negative_score, 'POLARITY SCORE': polarity_score,\n",
        "                                      'SUBJECTIVITY SCORE': subjectivity_score, 'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "                                      'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words, 'FOG INDEX': fog_index,\n",
        "                                      'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "                                      'SYLLABLE PER WORD': syllables_count, 'PERSONAL PRONOUNS': personal_pronouns_count,\n",
        "                                      'AVG WORD LENGTH': avg_word_length})\n",
        "\n",
        "        output_df.loc[len(output_df)] = new_row\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for URL_ID: {url_id}\")\n",
        "\n",
        "# Save the DataFrame to 'Output.xlsx'\n",
        "output_df.to_excel('Output Data Structure Self.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMrp-xG-wMXo",
        "outputId": "c0360e59-13a8-4a2e-8d50-6eb61f2147a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found for URL_ID: blackassign0036\n",
            "File not found for URL_ID: blackassign0049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMtIPsmaaa66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}